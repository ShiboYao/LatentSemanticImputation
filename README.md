#  Enhancing Domain Word Embedding via Latent Semantic Imputation  #

**Link to paper:[https://arxiv.org/pdf/1905.08900.pdf](https://arxiv.org/pdf/1905.08900.pdf)**

**The presentation slides can be found [here](https://github.com/ShiboYao/ShiboYao.github.io/blob/master/KDD19Yao.pdf).**

Overview 
======
 
We present a method to learn entity representations in a space based on their representations in another space. A direct application is to impute missing or unreliable word embeddings based on prior knowledge. A fairly similar problem is graph-based semi-supervised learning. 

### How do I get set up? ###

Requirements for replicating experiment results

+ Tensorflow
+ sklearn
+ Numpy


## Citation ##

If you utilize the idea and/or the code, please cite our paper:


```
@inproceedings{10.1145/3292500.3330926,
	author = {Yao, Shibo and Yu, Dantong and Xiao, Keli},
	title = {Enhancing Domain Word Embedding via Latent Semantic Imputation},
	year = {2019},
	isbn = {9781450362016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3292500.3330926},
	doi = {10.1145/3292500.3330926},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
	pages = {557–565},
	numpages = {9},
	keywords = {graph, manifold learning, representation learning, spectral methods},
	location = {Anchorage, AK, USA},
	series = {KDD ’19}
}
```

Contact
======

* Shibo Yao ([espoyao@gmail.com](espoyao@gmail.com))

